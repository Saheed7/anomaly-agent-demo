{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjKm5aNXAYypGy6ZiF/sSK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saheed7/anomaly-agent-demo/blob/main/Anomaly_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly_Agent_Demo"
      ],
      "metadata": {
        "id": "xRFFiM0iCbYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Dependencies\n",
        "!pip install pandas scikit-learn shap anthropic openai deepseek-py python-dotenv\n"
      ],
      "metadata": {
        "id": "FgKpSCfWCgJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Import Libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import shap\n",
        "import joblib\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "import anthropic\n",
        "from deepseek import DeepSeek"
      ],
      "metadata": {
        "id": "SB05Mpa7C36c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load API Keys (Create .env file with your keys)\n",
        "load_dotenv()\n",
        "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "claude_client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
        "deepseek_client = DeepSeek(api_key=os.getenv(\"DEEPSEEK_API_KEY\"))"
      ],
      "metadata": {
        "id": "RYt__4pcjX2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load Dataset (Synthetic example - replace with Edge-IIoTset/CIC-IoT 2023)\n",
        "data = {\n",
        "    'packet_size': np.random.randint(50, 1500, 1000),\n",
        "    'protocol_type': np.random.choice(['Modbus', 'MQTT', 'CoAP', 'HTTP'], 1000),\n",
        "    'packet_rate': np.random.uniform(1, 500, 1000),\n",
        "    'duration': np.random.uniform(0.1, 60, 1000),\n",
        "    'label': np.random.choice(['Benign', 'DDoS-UDP', 'PortScan', 'PLC-Injection'], 1000,\n",
        "                             p=[0.7, 0.1, 0.1, 0.1])\n",
        "}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "2KBWPfKGjyEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Preprocessing\n",
        "def preprocess_data(df):\n",
        "    # Encode categorical features\n",
        "    df = pd.get_dummies(df, columns=['protocol_type'])\n",
        "\n",
        "    # Normalize numerical features\n",
        "    scaler = MinMaxScaler()\n",
        "    numerical_features = ['packet_size', 'packet_rate', 'duration']\n",
        "    df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "    # Separate features and labels\n",
        "    X = df.drop('label', axis=1)\n",
        "    y = df['label']\n",
        "    return X, y, scaler\n",
        "\n",
        "X, y, scaler = preprocess_data(df)"
      ],
      "metadata": {
        "id": "2Y70Nqjzj4V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train Models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100),\n",
        "    'SVM': SVC(kernel='rbf', C=1.0, probability=True),\n",
        "    'Na√Øve Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X, y)"
      ],
      "metadata": {
        "id": "8zpc2jYNkFTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. SHAP Analysis\n",
        "explainer = shap.KernelExplainer(models['Random Forest'].predict_proba, shap.sample(X, 100))\n",
        "shap_values = explainer.shap_values(X)"
      ],
      "metadata": {
        "id": "i_TEMhqEkIiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. LLM Agent Functions\n",
        "def call_llm(llm_name, system_prompt, user_prompt):\n",
        "    \"\"\"Call different LLM APIs with standardized prompt structure\"\"\"\n",
        "    if llm_name == \"DeepSeek R1\":\n",
        "        response = deepseek_client.chat(\n",
        "            model=\"deepseek-chat\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            max_tokens=512\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    elif llm_name == \"GPT-4o\":\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            max_tokens=512\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    elif llm_name == \"GPT-4o\":\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            max_tokens=512\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    elif llm_name == \"Claude 3.5\":\n",
        "        response = claude_client.messages.create(\n",
        "            model=\"claude-3-5-sonnet-20240620\",\n",
        "            max_tokens=512,\n",
        "            system=system_prompt,\n",
        "            messages=[{\"role\": \"user\", \"content\": user_prompt}]\n",
        "        )\n",
        "        return response.content[0].text\n",
        "\n",
        "    elif llm_name == \"GPT-4o-mini\":\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            max_tokens=512\n",
        "        )\n",
        "        return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "YrtTVNpAkPgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Anomaly Agent Inference Pipeline\n",
        "def anomaly_agent_inference(features, llm_name=\"DeepSeek R1\", sensitivity=\"balanced\"):\n",
        "    # Preprocess input\n",
        "    input_df = pd.DataFrame([features])\n",
        "    input_df = pd.get_dummies(input_df)\n",
        "    input_df = input_df.reindex(columns=X.columns, fill_value=0)\n",
        "\n",
        "    # Get predictions from all models\n",
        "    predictions = {}\n",
        "    for name, model in models.items():\n",
        "        proba = model.predict_proba(input_df)[0]\n",
        "        top_class_idx = np.argsort(proba)[-1]\n",
        "        predictions[name] = {\n",
        "            \"prediction\": model.classes_[top_class_idx],\n",
        "            \"confidence\": float(proba[top_class_idx]),\n",
        "            \"details\": {model.classes_[i]: float(proba[i]) for i in range(len(proba))}\n",
        "        }\n",
        "    # Get SHAP explanation\n",
        "    shap_exp = explainer.shap_values(input_df)\n",
        "    shap_values_simplified = {\n",
        "        feature: float(value)\n",
        "        for feature, value in zip(X.columns, shap_exp[0][0])\n",
        "    }\n",
        "\n",
        "    # Prepare LLM prompt\n",
        "    system_prompt = f\"\"\"\n",
        "    You are an IoT security analyst. Analyze these model predictions and SHAP values to:\n",
        "    1. Determine final prediction (Benign/Malicious with attack type)\n",
        "    2. Explain reasoning using SHAP values\n",
        "    3. Apply {sensitivity} sensitivity mode\n",
        "    Output ONLY JSON with keys: prediction, confidence, explanation\n",
        "    \"\"\"\n",
        "    user_prompt = f\"\"\"\n",
        "    ## Model Predictions:\n",
        "    {json.dumps(predictions, indent=2)}\n",
        "\n",
        "    ## SHAP Values (Feature Contributions):\n",
        "    {json.dumps(shap_values_simplified, indent=2)}\n",
        "\n",
        "    ## Traffic Features:\n",
        "    {json.dumps(features, indent=2)}\n",
        "    \"\"\"\n",
        "    # Call LLM for final decision\n",
        "    llm_response = call_llm(llm_name, system_prompt, user_prompt)\n",
        "\n",
        "    try:\n",
        "        # Extract JSON from LLM response\n",
        "        json_start = llm_response.find('{')\n",
        "        json_end = llm_response.rfind('}') + 1\n",
        "        final_decision = json.loads(llm_response[json_start:json_end])\n",
        "        return final_decision\n",
        "    except:\n",
        "        return {\"error\": \"LLM response parsing failed\", \"raw_response\": llm_response}\n"
      ],
      "metadata": {
        "id": "FzeVCCoglv9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Demo Execution\n",
        "sample_features = {\n",
        "    'packet_size': 1200,\n",
        "    'packet_rate': 85.3,\n",
        "    'duration': 4.2,\n",
        "    'protocol_type': 'Modbus'\n",
        "}\n",
        "\n",
        "print(\"### DeepSeek R1 Results ###\")\n",
        "print(anomaly_agent_inference(sample_features, \"DeepSeek R1\"))\n",
        "\n",
        "print(\"\\n### GPT-4o Results ###\")\n",
        "print(anomaly_agent_inference(sample_features, \"GPT-4o\"))\n",
        "\n",
        "print(\"\\n### Claude 3.5 Results ###\")\n",
        "print(anomaly_agent_inference(sample_features, \"Claude 3.5\"))\n",
        "\n",
        "print(\"\\n### GPT-4o-mini Results ###\")\n",
        "print(anomaly_agent_inference(sample_features, \"GPT-4o-mini\"))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y2qsKH4VmkJ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}